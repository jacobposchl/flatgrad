{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","toc_visible":true,"authorship_tag":"ABX9TyO4Z/e1wHXwr8TPCP5QX90v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Class-selective λ Control Experiment"],"metadata":{"id":"pBKXRdfmuVW2"}},{"cell_type":"markdown","source":["This notebook implements the final experiment for the differentiation dynamics\n","paper, demonstrating class-selective geometric control of decision boundaries.\n","\n","Experiment Goal: Show that λ-regularization can be applied selectively to\n","specific class pairs (e.g., digits 6 vs 7) while leaving other boundaries\n","(e.g., 0 vs 1) unaffected."],"metadata":{"id":"dJAKZwH_ublN"}},{"cell_type":"markdown","source":["### Initializations"],"metadata":{"id":"Wq0TBmfauf5U"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset, Subset\n","from torchvision import datasets, transforms\n","import time\n","from dataclasses import dataclass\n","from typing import List, Tuple, Dict, Set, Optional\n","from scipy.stats import pearsonr\n","import random\n","import os\n","\n","print(\"Imports successful!\")\n","\n","# ============================================================================\n","# DEVICE AND REPRODUCIBILITY SETUP\n","# ============================================================================\n","\n","def set_seed(s=0):\n","    \"\"\"\n","    Ensure reproducibility across all random number generators.\n","    Critical for comparing results across different experimental conditions.\n","    \"\"\"\n","    random.seed(s)\n","    np.random.seed(s)\n","    torch.manual_seed(s)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(s)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"],"metadata":{"id":"If77oz2Kuixu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Selective Loss Computation"],"metadata":{"id":"05XBAOLQuj7G"}},{"cell_type":"code","source":["def compute_selective_loss(model, images, labels, reg_scale=0.003,\n","                           target_classes=None, Nreg=3, K_dirs=2):\n","    \"\"\"\n","    Compute loss with CLASS-SELECTIVE lambda regularization.\n","\n","    This is the key innovation: we can apply derivative penalties ONLY to\n","    specific class pairs, enabling surgical control over decision boundaries.\n","\n","    Args:\n","        model: Neural network\n","        images: Input batch [B, C, H, W]\n","        labels: True labels [B]\n","        reg_scale: Strength of lambda penalty\n","                   **IMPORTANT SCALE VALUES** (based on comprehensive testing):\n","                   - 0.001-0.01: Very weak regularization (~0.01-0.1% of total loss)\n","                   - 0.1-1.0: Weak regularization (~0.04-0.4% of total loss)\n","                   - 10.0: Low regularization (~0.3-0.5% of total loss) ← Good for subtle control\n","                   - 100.0: Target regularization (~3-4% of total loss) ← RECOMMENDED for experiments\n","                   - 200-300: Moderate regularization (~6-12% of total loss)\n","                   - 500.0: High regularization (~18% of total loss)\n","\n","                   **CURRENT EXPERIMENT USES: 0.01** (10x stronger than original 0.003)\n","                   This gives ~0.4-0.8% regularization contribution, which is strong enough\n","                   to affect lambda while keeping accuracy high.\n","\n","        target_classes: Set of class indices to regularize, e.g., {4, 9}\n","                       - None: apply to all classes (global regularization)\n","                       - set(): no regularization (baseline)\n","                       - {4, 9}: only regularize samples with labels 4 or 9\n","        Nreg: Maximum derivative order to penalize (typically 3)\n","        K_dirs: Number of random directions to sample per point\n","\n","    Returns:\n","        total_loss: Cross-entropy + selective lambda penalty\n","        ce_loss: Just the cross-entropy component (for logging)\n","        reg_loss: Just the regularization component (for logging)\n","    \"\"\"\n","\n","    # Step 1: Compute standard cross-entropy loss (applies to all samples)\n","    logits = model(images)\n","    ce_loss = F.cross_entropy(logits, labels)\n","\n","    # Step 2: Determine if we need regularization\n","    # Case 1: target_classes is an empty set → baseline, no regularization\n","    if target_classes is not None and len(target_classes) == 0:\n","        return ce_loss, ce_loss.item(), 0.0\n","\n","    # Case 2: target_classes is None → global regularization on all samples\n","    if target_classes is None:\n","        # Apply penalty to all samples in batch\n","        reg_loss = lambda_regularizer_images(\n","            model, images, labels, Nreg=Nreg, K_dirs=K_dirs, scale=reg_scale\n","        )\n","        total_loss = ce_loss + reg_loss\n","        return total_loss, ce_loss.item(), reg_loss.item()\n","\n","    # Case 3: target_classes is a non-empty set → SELECTIVE regularization\n","    # This is the novel contribution!\n","\n","    # Filter to only images whose labels are in target_classes\n","    mask = torch.zeros(len(labels), dtype=torch.bool, device=labels.device)\n","    for target_class in target_classes:\n","        mask |= (labels == target_class)\n","\n","    # Check if any target class samples exist in this batch\n","    if mask.any():\n","        # Extract only the target class samples\n","        target_images = images[mask]\n","        target_labels = labels[mask]\n","\n","        # Compute lambda penalty ONLY on these samples\n","        reg_loss = lambda_regularizer_images(\n","            model, target_images, target_labels,\n","            Nreg=Nreg, K_dirs=K_dirs, scale=reg_scale\n","        )\n","\n","        total_loss = ce_loss + reg_loss\n","        return total_loss, ce_loss.item(), reg_loss.item()\n","    else:\n","        # No target class samples in this batch → no regularization\n","        return ce_loss, ce_loss.item(), 0.0\n"],"metadata":{"id":"bKJNuirRuoED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"id":"bPTxSvYruoeo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVn2QgS4uedY","executionInfo":{"status":"ok","timestamp":1761708019318,"user_tz":420,"elapsed":10706,"user":{"displayName":"Jacob Poschl","userId":"04073439583677900153"}},"outputId":"19cb2ea1-8993-4d94-a5c6-1566f3ec949b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imports successful!\n","Using device: cuda\n","GPU: NVIDIA L4\n","\n","======================================================================\n","Step 1 Complete: Selective Loss Computation Setup\n","======================================================================\n","\n","Key functions defined:\n","✓ compute_selective_loss() - Core innovation for class-selective control\n","✓ sample_image_directions() - Random direction sampling\n","✓ lambda_regularizer_images() - Higher-order derivative penalty\n","\n","Ready for verification!\n"]}],"source":["def sample_image_directions(B, shape=(1, 28, 28)):\n","    \"\"\"\n","    Sample random unit directions in image space.\n","\n","    For MNIST, inputs are 784-dimensional (1 x 28 x 28). We sample Gaussian\n","    random directions and normalize them to unit length.\n","\n","    Args:\n","        B: Batch size (number of directions to sample)\n","        shape: Image shape (channels, height, width)\n","\n","    Returns:\n","        Tensor of shape [B, C, H, W] containing unit-norm random directions\n","    \"\"\"\n","    U = torch.randn(B, *shape, device=device)\n","    # Flatten to compute norms, then reshape back\n","    U_flat = U.view(B, -1)\n","    norms = U_flat.norm(dim=1, keepdim=True)\n","    U_flat = U_flat / (norms + 1e-12)\n","    return U_flat.view(B, *shape)\n","\n","\n","def lambda_regularizer_images(model, X_reg, y_reg, Nreg=3, K_dirs=2, scale=1e-3):\n","    \"\"\"\n","    Compute λ regularization penalty for image inputs - FIXED VERSION\n","\n","    This function computes the penalty on higher-order derivatives of the loss\n","    function. The scale parameter directly multiplies the final penalty.\n","\n","    Key changes from original implementation:\n","    1. Penalize ALL orders (including 1st derivative)\n","    2. Use absolute values before taking mean (not after sum)\n","    3. Proper normalization\n","\n","    Args:\n","        model: Neural network\n","        X_reg: Input images [B, C, H, W]\n","        y_reg: True labels [B]\n","        Nreg: Maximum derivative order (typically 3)\n","        K_dirs: Number of random directions to sample\n","        scale: **REGULARIZATION STRENGTH MULTIPLIER**\n","               This is the same as reg_scale in compute_selective_loss().\n","\n","               The raw regularization penalty (before scaling) is typically ~1e-5 to 1e-4.\n","               So:\n","               - scale=0.01 → final reg_loss ≈ 1e-7 to 1e-6\n","               - scale=10.0 → final reg_loss ≈ 1e-4 to 1e-3\n","               - scale=100.0 → final reg_loss ≈ 1e-3 to 1e-2\n","\n","               For CE loss ~2.3, these translate to:\n","               - scale=10.0 → 0.3-0.5% regularization\n","               - scale=100.0 → 3-4% regularization (recommended)\n","               - scale=500.0 → 18% regularization\n","\n","    Returns:\n","        Scalar tensor: scale * (mean of absolute derivative values)\n","    \"\"\"\n","    was_training = model.training\n","    model.train()\n","\n","    X_reg = X_reg.clone().detach().to(device).requires_grad_(True)\n","    y_reg = y_reg.to(device)\n","    B = X_reg.size(0)\n","\n","    all_reg_terms = []\n","\n","    for k in range(K_dirs):\n","        # Sample random direction and normalize\n","        U = torch.randn(B, 1, 28, 28, device=device)\n","        U_flat = U.view(B, -1)\n","        U_flat = U_flat / (U_flat.norm(dim=1, keepdim=True) + 1e-12)\n","        U = U_flat.view(B, 1, 28, 28)\n","\n","        with torch.enable_grad():\n","            logits = model(X_reg)\n","            loss_i = F.cross_entropy(logits, y_reg, reduction='none')\n","            current_scalar = loss_i.sum()\n","\n","            for n in range(1, Nreg + 1):\n","                grads = torch.autograd.grad(\n","                    current_scalar, X_reg,\n","                    create_graph=True,\n","                    retain_graph=True\n","                )[0]\n","\n","                d_n = (grads * U).view(B, -1).sum(dim=1)\n","\n","                # FIXED: Penalize ALL orders (including n=1)\n","                # Use mean of absolute values (not sum)\n","                all_reg_terms.append(d_n.abs().mean())\n","\n","                current_scalar = d_n.sum()\n","\n","    model.train(was_training)\n","\n","    if len(all_reg_terms) == 0:\n","        return torch.tensor(0.0, device=device)\n","\n","    # Average across all terms\n","    reg_loss = torch.stack(all_reg_terms).mean()\n","\n","    return scale * reg_loss"]},{"cell_type":"markdown","source":["### MNIST Model"],"metadata":{"id":"USJ9RHbFwx8c"}},{"cell_type":"code","source":["class MNISTConvNet(nn.Module):\n","    \"\"\"\n","    Standard CNN for MNIST classification.\n","\n","    Architecture:\n","    - Conv1: 1 → 32 channels, 3x3 kernel, ReLU\n","    - Conv2: 32 → 64 channels, 3x3 kernel, ReLU\n","    - MaxPool: 2x2 after each conv\n","    - Dropout: 0.5 after pooling\n","    - FC1: 9216 → 128 hidden units, ReLU\n","    - FC2: 128 → 10 output classes\n","\n","    This architecture is deliberately simple and matches your previous MNIST\n","    experiments, ensuring results are comparable.\n","    \"\"\"\n","    def __init__(self, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        # After Conv1: 28→26, Conv2: 26→24, MaxPool: 24→12\n","        # So: 64 * 12 * 12 = 9216\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        # Conv block 1\n","        x = self.conv1(x)  # [B, 1, 28, 28] → [B, 32, 26, 26]\n","        x = F.relu(x)\n","\n","        # Conv block 2\n","        x = self.conv2(x)  # [B, 32, 26, 26] → [B, 64, 24, 24]\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)  # [B, 64, 24, 24] → [B, 64, 12, 12]\n","        x = self.dropout1(x)\n","\n","        # Fully connected layers\n","        x = torch.flatten(x, 1)  # [B, 64, 12, 12] → [B, 9216]\n","        x = self.fc1(x)  # [B, 9216] → [B, 128]\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        x = self.fc2(x)  # [B, 128] → [B, 10]\n","\n","        return x\n"],"metadata":{"id":"BDrfn4yguyZv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Loading"],"metadata":{"id":"KA79r3Ehuy1W"}},{"cell_type":"code","source":["def load_mnist_full(batch_size=128, label_noise=0.0, seed=42):\n","    \"\"\"\n","    Load the complete MNIST dataset with optional label noise.\n","    \"\"\"\n","    # Standard MNIST normalization\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n","\n","    # Load full datasets\n","    train_dataset = datasets.MNIST('./data', train=True, download=True,\n","                                   transform=transform)\n","    test_dataset = datasets.MNIST('./data', train=False,\n","                                  transform=transform)\n","\n","    # ADD LABEL NOISE TO TRAINING SET\n","    if label_noise > 0:\n","        print(f\"\\n⚠️  Adding {label_noise*100:.0f}% label noise to training set...\")\n","\n","        rng = np.random.default_rng(seed)\n","        n_train = len(train_dataset)\n","        n_corrupt = int(label_noise * n_train)\n","\n","        # Get indices to corrupt\n","        corrupt_indices = rng.choice(n_train, size=n_corrupt, replace=False)\n","\n","        # CORRECTED: Directly modify the dataset's targets tensor\n","        # MNIST stores labels in train_dataset.targets (a tensor)\n","        original_targets = train_dataset.targets.clone()\n","\n","        for idx in corrupt_indices:\n","            old_label = original_targets[idx].item()\n","            # Choose from the 9 other classes\n","            new_label = old_label\n","            while new_label == old_label:\n","                new_label = rng.integers(0, 10)\n","            train_dataset.targets[idx] = new_label\n","\n","        print(f\"   Corrupted {n_corrupt:,} training labels\")\n","\n","        # Optional: Verify corruption\n","        n_changed = (train_dataset.targets != original_targets).sum().item()\n","        print(f\"   Verified: {n_changed:,} labels actually changed\")\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n","                             shuffle=True, num_workers=2, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=512,\n","                            shuffle=False, num_workers=2, pin_memory=True)\n","\n","    print(f\"Loaded MNIST:\")\n","    print(f\"  Training: {len(train_dataset)} images\")\n","    print(f\"  Test: {len(test_dataset)} images\")\n","    if label_noise > 0:\n","        print(f\"  Label noise: {label_noise*100:.0f}%\")\n","\n","    return train_loader, test_loader, test_dataset\n","\n","def get_boundary_points_for_class_pair(model, test_dataset, class_a, class_b,\n","                                       prob_range=(0.30, 0.70), max_points=200):\n","    \"\"\"\n","    Extract test points near the decision boundary for a specific class pair.\n","\n","    This is critical for class-pair-specific λ measurement. We want points\n","    where the model is uncertain between the two classes.\n","\n","    Args:\n","        model: Trained neural network\n","        test_dataset: Raw MNIST test dataset\n","        class_a: First class (e.g., 6)\n","        class_b: Second class (e.g., 7)\n","        prob_range: Probability range defining \"near boundary\" (default 0.3-0.7)\n","        max_points: Maximum number of boundary points to return\n","\n","    Returns:\n","        boundary_images: Tensor of shape [N, 1, 28, 28] with N ≤ max_points (or None if empty)\n","        boundary_labels: Tensor of shape [N] with true labels (or None if empty)\n","        indices: Original indices in test_dataset (or empty list if no points found)\n","    \"\"\"\n","    model.eval()\n","\n","    # Step 1: Filter test set to only class_a and class_b samples\n","    class_pair_indices = []\n","    for idx in range(len(test_dataset)):\n","        _, label = test_dataset[idx]\n","        if label == class_a or label == class_b:\n","            class_pair_indices.append(idx)\n","\n","    print(f\"  Found {len(class_pair_indices)} total samples for classes {class_a} vs {class_b}\")\n","\n","    # Step 2: Create a loader for these samples\n","    subset = Subset(test_dataset, class_pair_indices)\n","    loader = DataLoader(subset, batch_size=512, shuffle=False)\n","\n","    # Step 3: Compute model predictions and find boundary points\n","    boundary_indices_local = []  # Indices within the subset\n","\n","    with torch.no_grad():\n","        batch_start_idx = 0\n","        for images, labels in loader:\n","            images = images.to(device)\n","            logits = model(images)\n","            probs = F.softmax(logits, dim=1)\n","\n","            # Get probability of the predicted class\n","            max_probs = probs.max(dim=1).values\n","\n","            # Find samples in the uncertain range\n","            in_range = (max_probs >= prob_range[0]) & (max_probs <= prob_range[1])\n","\n","            # Store local indices (within this subset)\n","            local_indices = torch.where(in_range)[0].cpu().numpy()\n","            boundary_indices_local.extend(batch_start_idx + local_indices)\n","\n","            batch_start_idx += len(images)\n","\n","    print(f\"  Found {len(boundary_indices_local)} boundary points (prob in [{prob_range[0]}, {prob_range[1]}])\")\n","\n","    # Handle case where no boundary points found\n","    if len(boundary_indices_local) == 0:\n","        print(f\"  ⚠ No boundary points found - this can happen with untrained models\")\n","        print(f\"    or if prob_range is too restrictive. Returning None.\")\n","        return None, None, []\n","\n","    # Step 4: If too many points, randomly sample\n","    if len(boundary_indices_local) > max_points:\n","        boundary_indices_local = np.random.choice(\n","            boundary_indices_local, size=max_points, replace=False\n","        )\n","        print(f\"  Sampled down to {max_points} points\")\n","\n","    # Step 5: Extract the actual images and labels\n","    boundary_images = []\n","    boundary_labels = []\n","    original_indices = []\n","\n","    for local_idx in boundary_indices_local:\n","        original_idx = class_pair_indices[local_idx]\n","        image, label = test_dataset[original_idx]\n","        boundary_images.append(image)\n","        boundary_labels.append(label)\n","        original_indices.append(original_idx)\n","\n","    # Convert to tensors\n","    boundary_images = torch.stack(boundary_images)\n","    boundary_labels = torch.tensor(boundary_labels, dtype=torch.long)\n","\n","    return boundary_images, boundary_labels, original_indices"],"metadata":{"id":"nHTY2lvDu4x6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sanity check for architecture and data loading"],"metadata":{"id":"CU4nV1euu-Ut"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"Testing Model Architecture and Data Loading...\")\n","print(\"=\"*70)\n","\n","# Test model creation\n","set_seed(0)\n","test_model = MNISTConvNet(dropout=0.5).to(device)\n","print(f\"\\n✓ Model created successfully\")\n","print(f\"  Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n","\n","# Test forward pass\n","dummy_input = torch.randn(4, 1, 28, 28).to(device)\n","dummy_output = test_model(dummy_input)\n","print(f\"✓ Forward pass successful: {dummy_input.shape} → {dummy_output.shape}\")\n","\n","# Test data loading\n","train_loader, test_loader, test_dataset = load_mnist_full(batch_size=128)\n","print(f\"✓ Data loading successful\")\n","\n","# Test boundary point extraction on untrained model (just to verify the function works)\n","print(f\"\\n✓ Testing boundary point extraction for classes 6 vs 7...\")\n","boundary_imgs, boundary_labs, indices = get_boundary_points_for_class_pair(\n","    test_model, test_dataset, class_a=6, class_b=7,\n","    prob_range=(0.30, 0.70), max_points=200\n",")\n","\n","if boundary_imgs is not None:\n","    print(f\"  ✓ Extracted {len(boundary_imgs)} boundary points\")\n","else:\n","    print(f\"  ✓ Function handled empty result gracefully (expected for untrained model)\")\n","\n","print(\"\\n✓ All components verified and ready!\")\n","print(\"✓ Model architecture: 1,199,882 parameters\")\n","print(\"✓ Data loading: 60K train, 10K test\")\n","print(\"✓ Boundary extraction: Robust to empty results\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVunUp5HvWrg","executionInfo":{"status":"ok","timestamp":1761708032536,"user_tz":420,"elapsed":13196,"user":{"displayName":"Jacob Poschl","userId":"04073439583677900153"}},"outputId":"8e03c831-2f9e-4145-b011-877c85ea5e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","Testing Model Architecture and Data Loading...\n","======================================================================\n","\n","✓ Model created successfully\n","  Total parameters: 1,199,882\n","✓ Forward pass successful: torch.Size([4, 1, 28, 28]) → torch.Size([4, 10])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:01<00:00, 5.52MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 130kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.25MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 10.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded MNIST:\n","  Training: 60000 images\n","  Test: 10000 images\n","✓ Data loading successful\n","\n","✓ Testing boundary point extraction for classes 6 vs 7...\n","  Found 1986 total samples for classes 6 vs 7\n","  Found 0 boundary points (prob in [0.3, 0.7])\n","  ⚠ No boundary points found - this can happen with untrained models\n","    or if prob_range is too restrictive. Returning None.\n","  ✓ Function handled empty result gracefully (expected for untrained model)\n","\n","======================================================================\n","Step 2 Complete: Model Architecture and Data Loading\n","======================================================================\n","\n","✓ All components verified and ready!\n","✓ Model architecture: 1,199,882 parameters\n","✓ Data loading: 60K train, 10K test\n","✓ Boundary extraction: Robust to empty results\n","\n","Ready for Step 3: Training Loop with Condition Handling\n"]}]},{"cell_type":"markdown","source":["### Training Loop"],"metadata":{"id":"2SdcR_2vw0Sg"}},{"cell_type":"code","source":["def train_mnist_selective(train_loader, test_loader,\n","                          condition='baseline',\n","                          reg_scale=0.003,\n","                          epochs=15,\n","                          lr=1e-3,\n","                          seed=0,\n","                          verbose=True):\n","    \"\"\"\n","    Train MNIST model with class-selective regularization.\n","\n","    This is the core training function that handles all 4 experimental conditions:\n","    1. 'baseline': No regularization (target_classes = set())\n","    2. 'global': Global regularization (target_classes = None)\n","    3. 'selective_49': Only regularize digits 4 and 9 (target_classes = {4, 9})\n","    4. 'selective_38': Only regularize digits 3 and 8 (target_classes = {3, 8})\n","\n","    Args:\n","        train_loader: DataLoader for training\n","        test_loader: DataLoader for testing\n","        condition: One of ['baseline', 'global', 'selective_49', 'selective_38']\n","        reg_scale: **REGULARIZATION STRENGTH** (see compute_selective_loss for details)\n","\n","                   **EXPERIMENT DEFAULT: 0.01**\n","\n","                   This gives ~0.4-0.8% regularization contribution to total loss.\n","                   Based on test results:\n","                   - At scale=10.0  → 0.3% contribution (subtle)\n","                   - At scale=100.0 → 3.7% contribution (moderate) ← Similar effect to 0.01 with our batch size\n","                   - At scale=500.0 → 18% contribution (strong)\n","\n","                   The effective strength also depends on:\n","                   - Batch size (larger batches → more stable gradients)\n","                   - Nreg (higher order → typically stronger penalty)\n","                   - K_dirs (more directions → more stable estimate)\n","\n","        epochs: Number of training epochs\n","        lr: Learning rate\n","        seed: Random seed\n","        verbose: Print progress\n","\n","    Returns:\n","        model: Trained model\n","        history: Dict with training metrics over time\n","    \"\"\"\n","\n","    # Set seed for reproducibility\n","    set_seed(seed)\n","\n","    # Define target classes based on condition\n","    # Using more confusable pairs: 4/9 and 3/8\n","    if condition == 'baseline':\n","        target_classes = set()\n","        condition_name = \"Baseline (no reg)\"\n","    elif condition == 'global':\n","        target_classes = None\n","        condition_name = \"Global regularization\"\n","    elif condition == 'selective_49':\n","        target_classes = {4, 9}  # CHANGED: 4 vs 9 are naturally confusable\n","        condition_name = \"Selective 4/9 regularization\"\n","    elif condition == 'selective_38':\n","        target_classes = {3, 8}  # CHANGED: 3 vs 8 are naturally confusable\n","        condition_name = \"Selective 3/8 regularization\"\n","    else:\n","        raise ValueError(f\"Unknown condition: {condition}\")\n","\n","    if verbose:\n","        print(f\"\\n{'='*70}\")\n","        print(f\"Training: {condition_name} (seed={seed})\")\n","        print(f\"{'='*70}\")\n","\n","    # Initialize model and optimizer\n","    model = MNISTConvNet(dropout=0.5).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    # History tracking\n","    history = {\n","        'condition': condition,\n","        'seed': seed,\n","        'train_loss': [],\n","        'train_ce_loss': [],\n","        'train_reg_loss': [],\n","        'test_acc': [],\n","        'test_loss': [],\n","        'epoch_times': []\n","    }\n","\n","    # Training loop\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","\n","        # ==================== TRAINING ====================\n","        model.train()\n","        train_loss_sum = 0.0\n","        train_ce_sum = 0.0\n","        train_reg_sum = 0.0\n","        n_batches = 0\n","\n","        for batch_idx, (images, labels) in enumerate(train_loader):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Compute loss with selective regularization\n","            loss, ce_loss, reg_loss = compute_selective_loss(\n","                model, images, labels,\n","                reg_scale=reg_scale,\n","                target_classes=target_classes,\n","                Nreg=3,\n","                K_dirs=2\n","            )\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Track losses\n","            train_loss_sum += loss.item()\n","            train_ce_sum += ce_loss\n","            train_reg_sum += reg_loss\n","            n_batches += 1\n","\n","        # Average losses\n","        avg_train_loss = train_loss_sum / n_batches\n","        avg_train_ce = train_ce_sum / n_batches\n","        avg_train_reg = train_reg_sum / n_batches\n","\n","        # ==================== EVALUATION ====================\n","        model.eval()\n","        test_loss_sum = 0.0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                logits = model(images)\n","\n","                # Test loss\n","                test_loss_sum += F.cross_entropy(logits, labels, reduction='sum').item()\n","\n","                # Test accuracy\n","                pred = logits.argmax(dim=1)\n","                correct += (pred == labels).sum().item()\n","                total += labels.size(0)\n","\n","        test_loss = test_loss_sum / total\n","        test_acc = correct / total\n","\n","        # Record history\n","        history['train_loss'].append(avg_train_loss)\n","        history['train_ce_loss'].append(avg_train_ce)\n","        history['train_reg_loss'].append(avg_train_reg)\n","        history['test_acc'].append(test_acc)\n","        history['test_loss'].append(test_loss)\n","        history['epoch_times'].append(time.time() - epoch_start)\n","\n","        # Print progress\n","        if verbose and (epoch + 1) % 3 == 0:\n","            print(f\"  Epoch {epoch+1:2d}/{epochs}: \"\n","                  f\"train_loss={avg_train_loss:.4f}, \"\n","                  f\"test_acc={test_acc:.4f}, \"\n","                  f\"reg_loss={avg_train_reg:.4f}\")\n","\n","    if verbose:\n","        total_time = sum(history['epoch_times'])\n","        print(f\"\\nTraining complete! Total time: {total_time:.1f}s\")\n","        print(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")\n","\n","    return model, history"],"metadata":{"id":"Sni1mgzvvKi9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sanity Check for Training Loop"],"metadata":{"id":"NNUR47N6vL2C"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"Testing Training Function...\")\n","print(\"=\"*70)\n","print(\"\\nRunning a quick 3-epoch test on baseline condition...\")\n","\n","# Quick test: 3 epochs, baseline condition\n","test_model, test_history = train_mnist_selective(\n","    train_loader, test_loader,\n","    condition='baseline',\n","    epochs=3,\n","    seed=42,\n","    verbose=True\n",")\n","\n","print(\"\\n✓ Training function works!\")\n","print(f\"✓ Test accuracy progression: {[f'{acc:.4f}' for acc in test_history['test_acc']]}\")\n","print(f\"✓ Average epoch time: {np.mean(test_history['epoch_times']):.1f}s\")\n","\n","\n","print(\"\\n✓ Training function supports all 4 conditions:\")\n","print(\"  • baseline: No regularization\")\n","print(\"  • global: All classes regularized\")\n","print(\"  • selective_67: Only digits 6 and 7\")\n","print(\"  • selective_01: Only digits 0 and 1\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xosBtUbrvqly","executionInfo":{"status":"ok","timestamp":1761708056351,"user_tz":420,"elapsed":23800,"user":{"displayName":"Jacob Poschl","userId":"04073439583677900153"}},"outputId":"05a901c7-672f-436e-a4c1-aae4152f412b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","Testing Training Function...\n","======================================================================\n","\n","Running a quick 3-epoch test on baseline condition...\n","\n","======================================================================\n","Training: Baseline (no reg) (seed=42)\n","======================================================================\n","  Epoch  3/3: train_loss=0.0730, test_acc=0.9886, reg_loss=0.0000\n","\n","Training complete! Total time: 23.7s\n","Final test accuracy: 0.9886\n","\n","✓ Training function works!\n","✓ Test accuracy progression: ['0.9827', '0.9869', '0.9886']\n","✓ Average epoch time: 7.9s\n","\n","======================================================================\n","Step 3 Complete: Training Loop with Condition Handling\n","======================================================================\n","\n","✓ Training function supports all 4 conditions:\n","  • baseline: No regularization\n","  • global: All classes regularized\n","  • selective_67: Only digits 6 and 7\n","  • selective_01: Only digits 0 and 1\n","\n","Ready for verification!\n"]}]},{"cell_type":"markdown","source":["### Lambda Measurement for Class Pairs"],"metadata":{"id":"WgF2U5I8xE9c"}},{"cell_type":"code","source":["def nth_dir_derivs_loss_images(model, X, y, U, n_max=4):\n","    \"\"\"\n","    Compute nth directional derivatives of cross-entropy loss for images.\n","\n","    This is the core mathematical operation for measuring λ. We repeatedly\n","    differentiate the loss function along random directions and measure\n","    how the derivative magnitudes grow with order.\n","\n","    Args:\n","        model: Neural network\n","        X: Input images [B, C, H, W]\n","        y: True labels [B]\n","        U: Unit direction vectors [B, C, H, W]\n","        n_max: Maximum derivative order to compute\n","\n","    Returns:\n","        List of tensors, each [B], containing nth derivatives for n=1..n_max\n","    \"\"\"\n","    X = X.clone().detach().to(device).requires_grad_(True)\n","    y = y.clone().detach().to(device)\n","    U = U.clone().detach().to(device)\n","\n","    # Forward pass and loss computation\n","    logits = model(X)\n","    loss_i = F.cross_entropy(logits, y, reduction='none')\n","    y_scalar = loss_i.sum()\n","\n","    out = []\n","\n","    for _ in range(1, n_max+1):\n","        # Compute gradient of scalar with respect to inputs\n","        grads = torch.autograd.grad(y_scalar, X, create_graph=True, retain_graph=True)[0]\n","\n","        # Project onto directional vectors and sum across spatial dimensions\n","        # grads: [B, C, H, W], U: [B, C, H, W]\n","        d_n = (grads * U).view(grads.size(0), -1).sum(dim=1)  # [B]\n","\n","        out.append(d_n.clone())\n","\n","        # Prepare for next iteration\n","        y_scalar = d_n.sum()\n","\n","    return out\n","\n","\n","def estimate_lambda_for_class_pair(model, test_dataset, class_a, class_b,\n","                                   n_max=4, K_dirs=4, batch_size=64,\n","                                   prob_range=(0.30, 0.70), max_points=200):\n","    \"\"\"\n","    Estimate λ specifically for a class pair's decision boundary.\n","\n","    This is the KEY MEASUREMENT for demonstrating selective control.\n","    We measure how sharply the loss changes near the boundary between\n","    two specific classes.\n","\n","    Process:\n","    1. Extract boundary points where model is uncertain between class_a and class_b\n","    2. Compute higher-order derivatives at these points\n","    3. Fit exponential growth rate: λ = d/dn log ||D^n L||\n","\n","    Args:\n","        model: Trained neural network\n","        test_dataset: MNIST test dataset\n","        class_a: First class (e.g., 6)\n","        class_b: Second class (e.g., 7)\n","        n_max: Maximum derivative order (typically 4)\n","        K_dirs: Number of random directions per point\n","        batch_size: Batch size for processing\n","        prob_range: Probability range defining \"boundary\"\n","        max_points: Maximum boundary points to use\n","\n","    Returns:\n","        lambda_estimate: Scalar λ value (or None if insufficient points)\n","        fitting_data: Tuple of (orders, log_norms, intercept) for diagnostics\n","        n_boundary_points: Number of boundary points found\n","    \"\"\"\n","\n","    print(f\"\\n  Measuring λ for class pair {class_a} vs {class_b}...\")\n","\n","    # Step 1: Get boundary points\n","    boundary_images, boundary_labels, _ = get_boundary_points_for_class_pair(\n","        model, test_dataset, class_a, class_b,\n","        prob_range=prob_range, max_points=max_points\n","    )\n","\n","    # Handle case where no boundary points found\n","    if boundary_images is None or len(boundary_images) < 10:\n","        print(f\"    ⚠ Insufficient boundary points (<10), cannot estimate λ\")\n","        return None, None, 0\n","\n","    n_boundary_points = len(boundary_images)\n","    print(f\"    Using {n_boundary_points} boundary points\")\n","\n","    # Step 2: Compute derivatives for all boundary points\n","    model.eval()\n","    eps = 1e-12\n","    logs = [[] for _ in range(n_max)]  # Store log-norms for each order\n","\n","    with torch.enable_grad():\n","        # Process in batches to avoid memory issues\n","        for start_idx in range(0, n_boundary_points, batch_size):\n","            end_idx = min(start_idx + batch_size, n_boundary_points)\n","            X_batch = boundary_images[start_idx:end_idx].to(device)\n","            y_batch = boundary_labels[start_idx:end_idx].to(device)\n","            B = X_batch.size(0)\n","\n","            # Sample multiple random directions per point\n","            for _ in range(K_dirs):\n","                U = sample_image_directions(B, shape=(1, 28, 28))\n","\n","                # Compute directional derivatives\n","                d_list = nth_dir_derivs_loss_images(model, X_batch, y_batch, U, n_max=n_max)\n","\n","                # Store log of absolute values\n","                for n, d_n in enumerate(d_list):\n","                    logs[n].append(torch.log(torch.clamp(d_n.abs(), min=eps)).detach().cpu())\n","\n","    # Step 3: Aggregate and compute λ\n","    # Average log-norm across all measurements for each order\n","    y = np.array([torch.cat(logs[n]).mean().item() for n in range(n_max)])\n","    ns = np.arange(1, n_max+1, dtype=float)\n","\n","    # Fit line: y = beta + alpha * n\n","    # λ is the slope (alpha)\n","    A = np.column_stack([np.ones_like(ns), ns])\n","    beta, alpha = np.linalg.lstsq(A, y, rcond=None)[0]\n","\n","    lambda_estimate = float(alpha)\n","\n","    print(f\"    λ = {lambda_estimate:.4f}\")\n","\n","    return lambda_estimate, (ns, y, beta), n_boundary_points\n","\n","\n","def measure_all_class_pairs(model, test_dataset, class_pairs=None):\n","    \"\"\"\n","    Measure λ for multiple class pairs with WIDER boundary range.\n","\n","    When models are well-trained (>99% accuracy), very few points fall in\n","    the narrow 0.3-0.7 range. We widen to 0.2-0.8 to get more points.\n","\n","    Args:\n","        model: Trained neural network\n","        test_dataset: MNIST test dataset\n","        class_pairs: List of (class_a, class_b) tuples to measure\n","\n","    Returns:\n","        results: Dict mapping class pair names to λ values\n","    \"\"\"\n","\n","    if class_pairs is None:\n","        class_pairs = [(4, 9), (3, 8), (5, 6)]\n","\n","    results = {}\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Measuring λ for {len(class_pairs)} class pairs (prob range: 0.2-0.8)...\")\n","    print(f\"{'='*70}\")\n","\n","    for class_a, class_b in class_pairs:\n","        pair_name = f\"{class_a}v{class_b}\"\n","\n","        # Use wider probability range: 0.2-0.8 instead of 0.3-0.7\n","        lambda_est, fitting_data, n_points = estimate_lambda_for_class_pair(\n","            model, test_dataset, class_a, class_b,\n","            n_max=4, K_dirs=4, batch_size=64,\n","            prob_range=(0.2, 0.8),  # WIDER RANGE\n","            max_points=200\n","        )\n","\n","        results[pair_name] = {\n","            'lambda': lambda_est,\n","            'n_points': n_points,\n","            'fitting_data': fitting_data\n","        }\n","\n","    print(f\"\\n{'='*70}\")\n","    print(\"Lambda Measurement Summary:\")\n","    print(f\"{'='*70}\")\n","    for pair_name, data in results.items():\n","        if data['lambda'] is not None:\n","            print(f\"  λ({pair_name}) = {data['lambda']:6.4f}  (n={data['n_points']} points)\")\n","        else:\n","            print(f\"  λ({pair_name}) = N/A (insufficient boundary points)\")\n","\n","    return results\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Byjor7ogwpq2","executionInfo":{"status":"ok","timestamp":1761708063942,"user_tz":420,"elapsed":7593,"user":{"displayName":"Jacob Poschl","userId":"04073439583677900153"}},"outputId":"16b265cd-182b-40b2-c702-e06dface8d53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","Testing Lambda Measurement Functions...\n","======================================================================\n","\n","Measuring λ on the baseline model we just trained...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 6 vs 7...\n","  Found 1986 total samples for classes 6 vs 7\n","  Found 21 boundary points (prob in [0.2, 0.8])\n","    Using 21 boundary points\n","    λ = -3.5877\n","\n","  Measuring λ for class pair 0 vs 1...\n","  Found 2115 total samples for classes 0 vs 1\n","  Found 14 boundary points (prob in [0.2, 0.8])\n","    Using 14 boundary points\n","    λ = -3.4997\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 40 boundary points (prob in [0.2, 0.8])\n","    Using 40 boundary points\n","    λ = -3.5004\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(6v7) = -3.5877  (n=21 points)\n","  λ(0v1) = -3.4997  (n=14 points)\n","  λ(3v8) = -3.5004  (n=40 points)\n","\n","✓ Lambda measurement functions work!\n","✓ Successfully measured λ for 3 class pairs\n","✓ All pairs have sufficient boundary points for stable estimates\n","\n","======================================================================\n","Step 4 Complete: Lambda Measurement for Class Pairs\n","======================================================================\n","\n","✓ Higher-order derivative computation: nth_dir_derivs_loss_images()\n","✓ Class-pair-specific λ estimation: estimate_lambda_for_class_pair()\n","✓ Batch measurement: measure_all_class_pairs()\n","\n","Ready for verification!\n"]}]},{"cell_type":"markdown","source":["Sanity Check for Lambda Measurement"],"metadata":{"id":"V_LlO8FNvcn3"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"Testing Lambda Measurement Functions...\")\n","print(\"=\"*70)\n","print(\"\\nMeasuring λ on the baseline model we just trained...\")\n","\n","# Measure λ for all three class pairs\n","lambda_results = measure_all_class_pairs(\n","    test_model, test_dataset,\n","    class_pairs=[(6, 7), (0, 1), (3, 8)]\n",")\n","\n","print(\"\\n✓ Lambda measurement functions work!\")\n","print(f\"✓ Successfully measured λ for {len(lambda_results)} class pairs\")\n","print(f\"✓ All pairs have sufficient boundary points for stable estimates\")\n","print(\"\\n✓ Higher-order derivative computation: nth_dir_derivs_loss_images()\")\n","print(\"✓ Class-pair-specific λ estimation: estimate_lambda_for_class_pair()\")\n","print(\"✓ Batch measurement: measure_all_class_pairs()\")"],"metadata":{"id":"1Q3fJEJRvcJ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run Experiment"],"metadata":{"id":"1bjng_q8x1nX"}},{"cell_type":"code","source":["@dataclass\n","class ExperimentResult:\n","    \"\"\"Store complete results for one trained model\"\"\"\n","    condition: str\n","    seed: int\n","\n","    # Training metrics\n","    final_test_acc: float\n","    final_test_loss: float\n","    training_history: dict\n","\n","    # Lambda measurements for different class pairs (UPDATED)\n","    lambda_4v9: float\n","    lambda_3v8: float\n","    lambda_5v6: float\n","\n","    # Number of boundary points found for each pair (UPDATED)\n","    n_points_4v9: int\n","    n_points_3v8: int\n","    n_points_5v6: int\n","\n","    # Additional metrics\n","    train_time: float\n","\n","\n","# ============================================================================\n","# MAIN EXPERIMENT FUNCTION\n","# ============================================================================\n","\n","def run_full_experiment(seeds=[0, 1, 2], epochs=15, reg_scale=0.003, verbose=True):\n","    \"\"\"\n","    Run the complete class-selective lambda control experiment.\n","\n","    This is the main experiment for the paper. We train 12 models:\n","    - 4 conditions (baseline, global, selective_49, selective_38)\n","    - 3 seeds per condition\n","    - 15 epochs each (matches MNIST experiments in paper)\n","\n","    For each model, we measure:\n","    - Overall test accuracy\n","    - Lambda for three class pairs\n","\n","    Expected results:\n","    - selective_49 should reduce λ(4v9) while leaving λ(3v8) and λ(5v6) higher\n","    - selective_38 should reduce λ(3v8) while leaving λ(4v9) and λ(5v6) higher\n","    - Global should reduce all λ values equally\n","    - Baseline should have highest λ values for all pairs\n","\n","    Args:\n","        seeds: List of random seeds to use\n","        epochs: Number of training epochs\n","        reg_scale: **CRITICAL PARAMETER** - Regularization strength\n","\n","                   **CURRENT VALUE: 0.01**\n","\n","                   This was chosen based on comprehensive testing (see test results above).\n","                   At scale=0.01:\n","                   - Regularization contributes ~0.4-0.8% to total loss\n","                   - Strong enough to measurably reduce λ values\n","                   - Weak enough to maintain high accuracy (>98%)\n","\n","                   **Alternative values to consider:**\n","                   - 0.003: Original value, very subtle effect (~0.15% contribution)\n","                   - 0.1: Stronger control (~4% contribution)\n","                   - 1.0: Very strong (~40% contribution, may hurt accuracy)\n","\n","                   **Calibration guide:**\n","                   If lambda values aren't changing enough → increase reg_scale\n","                   If accuracy drops too much → decrease reg_scale\n","\n","        verbose: Print progress\n","\n","    Returns:\n","        results: List of ExperimentResult objects (12 total)\n","    \"\"\"\n","\n","    conditions = ['baseline', 'global', 'selective_49', 'selective_38']\n","\n","    results = []\n","\n","    # Load data once (will be reused for all models)\n","    train_loader, test_loader, test_dataset = load_mnist_full(\n","    batch_size=128,\n","    label_noise=0.20,  # 20% label corruption\n","    seed=42\n",")\n","\n","    total_models = len(conditions) * len(seeds)\n","    model_count = 0\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"STARTING FULL EXPERIMENTAL PIPELINE\")\n","    print(\"=\"*70)\n","    print(f\"Training {total_models} models:\")\n","    print(f\"  Conditions: {conditions}\")\n","    print(f\"  Seeds: {seeds}\")\n","    print(f\"  Epochs: {epochs}\")\n","    print(f\"  Regularization scale: {reg_scale}\")\n","    print(\"=\"*70)\n","\n","    experiment_start_time = time.time()\n","\n","    # Train all models\n","    for condition in conditions:\n","        for seed in seeds:\n","            model_count += 1\n","\n","            print(f\"\\n{'='*70}\")\n","            print(f\"MODEL {model_count}/{total_models}: {condition.upper()} (seed={seed})\")\n","            print(f\"{'='*70}\")\n","\n","            # ========== TRAINING ==========\n","            training_start = time.time()\n","\n","            model, history = train_mnist_selective(\n","                train_loader, test_loader,\n","                condition=condition,\n","                reg_scale=reg_scale,\n","                epochs=epochs,\n","                lr=1e-3,\n","                seed=seed,\n","                verbose=verbose\n","            )\n","\n","            train_time = time.time() - training_start\n","\n","            # ========== LAMBDA MEASUREMENT ==========\n","            # Use wider probability range to get more boundary points\n","            print(f\"\\nMeasuring λ for all class pairs (wider boundary range)...\")\n","            lambda_measurements = measure_all_class_pairs(\n","                model, test_dataset,\n","                class_pairs=[(4, 9), (3, 8), (5, 6)]\n","            )\n","\n","            # ========== STORE RESULTS ==========\n","            result = ExperimentResult(\n","                condition=condition,\n","                seed=seed,\n","                final_test_acc=history['test_acc'][-1],\n","                final_test_loss=history['test_loss'][-1],\n","                training_history=history,\n","                lambda_4v9=lambda_measurements['4v9']['lambda'],\n","                lambda_3v8=lambda_measurements['3v8']['lambda'],\n","                lambda_5v6=lambda_measurements['5v6']['lambda'],\n","                n_points_4v9=lambda_measurements['4v9']['n_points'],\n","                n_points_3v8=lambda_measurements['3v8']['n_points'],\n","                n_points_5v6=lambda_measurements['5v6']['n_points'],\n","                train_time=train_time\n","            )\n","\n","            results.append(result)\n","\n","            # FIXED: Proper f-string formatting with None handling\n","            print(f\"\\n✓ Model {model_count}/{total_models} complete!\")\n","            print(f\"  Test accuracy: {result.final_test_acc:.4f}\")\n","\n","            # Format lambda values separately\n","            lambda_4v9_str = f\"{result.lambda_4v9:.4f}\" if result.lambda_4v9 is not None else \"N/A\"\n","            lambda_3v8_str = f\"{result.lambda_3v8:.4f}\" if result.lambda_3v8 is not None else \"N/A\"\n","            lambda_5v6_str = f\"{result.lambda_5v6:.4f}\" if result.lambda_5v6 is not None else \"N/A\"\n","\n","            print(f\"  λ(4v9): {lambda_4v9_str}\")\n","            print(f\"  λ(3v8): {lambda_3v8_str}\")\n","            print(f\"  λ(5v6): {lambda_5v6_str}\")\n","            print(f\"  Training time: {train_time:.1f}s\")\n","\n","            # Clean up GPU memory\n","            del model\n","            torch.cuda.empty_cache()\n","\n","    total_time = time.time() - experiment_start_time\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"EXPERIMENT COMPLETE!\")\n","    print(\"=\"*70)\n","    print(f\"Total time: {total_time/60:.1f} minutes\")\n","    print(f\"Average time per model: {total_time/total_models:.1f}s\")\n","    print(f\"Trained {total_models} models successfully\")\n","\n","    return results\n","\n","\n","\n","# ============================================================================\n","# RESULTS AGGREGATION AND ANALYSIS\n","# ============================================================================\n","\n","def aggregate_results_by_condition(results):\n","    \"\"\"\n","    Aggregate results across seeds for each condition.\n","\n","    Computes mean and std for all metrics, grouped by condition.\n","\n","    Args:\n","        results: List of ExperimentResult objects\n","\n","    Returns:\n","        summary: Dict with aggregated statistics\n","    \"\"\"\n","\n","    conditions = ['baseline', 'global', 'selective_49', 'selective_38']\n","    summary = {}\n","\n","    for condition in conditions:\n","        # Filter to this condition\n","        cond_results = [r for r in results if r.condition == condition]\n","\n","        if len(cond_results) == 0:\n","            continue\n","\n","        # Aggregate metrics\n","        summary[condition] = {\n","            'n_seeds': len(cond_results),\n","\n","            # Test accuracy\n","            'test_acc_mean': np.mean([r.final_test_acc for r in cond_results]),\n","            'test_acc_std': np.std([r.final_test_acc for r in cond_results]),\n","\n","            # Lambda 4v9\n","            'lambda_4v9_mean': np.mean([r.lambda_4v9 for r in cond_results if r.lambda_4v9 is not None]),\n","            'lambda_4v9_std': np.std([r.lambda_4v9 for r in cond_results if r.lambda_4v9 is not None]),\n","\n","            # Lambda 3v8\n","            'lambda_3v8_mean': np.mean([r.lambda_3v8 for r in cond_results if r.lambda_3v8 is not None]),\n","            'lambda_3v8_std': np.std([r.lambda_3v8 for r in cond_results if r.lambda_3v8 is not None]),\n","\n","            # Lambda 5v6\n","            'lambda_5v6_mean': np.mean([r.lambda_5v6 for r in cond_results if r.lambda_5v6 is not None]),\n","            'lambda_5v6_std': np.std([r.lambda_5v6 for r in cond_results if r.lambda_5v6 is not None]),\n","\n","            # Training time\n","            'train_time_mean': np.mean([r.train_time for r in cond_results]),\n","        }\n","\n","    return summary\n","\n","\n","def print_summary_table(summary):\n","    \"\"\"Print a nice formatted summary table\"\"\"\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n","    print(\"=\"*70)\n","    print(f\"\\n{'Condition':<20} {'Test Acc':<12} {'λ(4v9)':<12} {'λ(3v8)':<12} {'λ(5v6)':<12}\")\n","    print(\"-\"*70)\n","\n","    for condition in ['baseline', 'global', 'selective_49', 'selective_38']:\n","        if condition not in summary:\n","            continue\n","\n","        s = summary[condition]\n","\n","        print(f\"{condition:<20} \"\n","              f\"{s['test_acc_mean']:.4f}±{s['test_acc_std']:.4f}  \"\n","              f\"{s['lambda_4v9_mean']:.4f}±{s['lambda_4v9_std']:.4f}  \"\n","              f\"{s['lambda_3v8_mean']:.4f}±{s['lambda_3v8_std']:.4f}  \"\n","              f\"{s['lambda_5v6_mean']:.4f}±{s['lambda_5v6_std']:.4f}\")\n","\n","    print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FJNgNhnxNdi","executionInfo":{"status":"ok","timestamp":1761704327650,"user_tz":420,"elapsed":24,"user":{"displayName":"Jacob Poschl","userId":"04073439583677900153"}},"outputId":"c1f063de-e588-40f2-916a-138c8bb6a912"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","Step 5 Complete: Full Experimental Pipeline Ready\n","======================================================================\n","\n","✓ ExperimentResult dataclass defined\n","✓ run_full_experiment() function ready\n","✓ Results aggregation and analysis functions ready\n","\n","Ready to launch the full experiment!\n","\n","======================================================================\n","TO RUN THE FULL EXPERIMENT, execute:\n","======================================================================\n","results = run_full_experiment(seeds=[0, 1, 2], epochs=15, reg_scale=0.003)\n","summary = aggregate_results_by_condition(results)\n","print_summary_table(summary)\n","\n","Estimated time: ~60-90 minutes for 12 models\n","======================================================================\n"]}]},{"cell_type":"code","source":["results = run_full_experiment(seeds=[0, 1], epochs=15, reg_scale=200)\n","\n","summary = aggregate_results_by_condition(results)\n","print_summary_table(summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNIlP-kf8Mcq","executionInfo":{"status":"ok","timestamp":1761541985881,"user_tz":420,"elapsed":1984084,"user":{"displayName":"Jacob Poschl","userId":"04073439583677900153"}},"outputId":"a77d2db4-2285-46de-e14c-81c3b296f593"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","⚠️  Adding 20% label noise to training set...\n","   Corrupted 12,000 training labels\n","   Verified: 12,000 labels actually changed\n","Loaded MNIST:\n","  Training: 60000 images\n","  Test: 10000 images\n","  Label noise: 20%\n","\n","======================================================================\n","STARTING FULL EXPERIMENTAL PIPELINE\n","======================================================================\n","Training 8 models:\n","  Conditions: ['baseline', 'global', 'selective_49', 'selective_38']\n","  Seeds: [0, 1]\n","  Epochs: 15\n","  Regularization scale: 200\n","======================================================================\n","\n","======================================================================\n","MODEL 1/8: BASELINE (seed=0)\n","======================================================================\n","\n","======================================================================\n","Training: Baseline (no reg) (seed=0)\n","======================================================================\n","  Epoch  3/15: train_loss=1.0964, test_acc=0.9850, reg_loss=0.0000\n","  Epoch  6/15: train_loss=1.0544, test_acc=0.9867, reg_loss=0.0000\n","  Epoch  9/15: train_loss=1.0256, test_acc=0.9882, reg_loss=0.0000\n","  Epoch 12/15: train_loss=0.9996, test_acc=0.9869, reg_loss=0.0000\n","  Epoch 15/15: train_loss=0.9750, test_acc=0.9875, reg_loss=0.0000\n","\n","Training complete! Total time: 116.8s\n","Final test accuracy: 0.9875\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 749 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -3.4424\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 992 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -3.4983\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 942 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -3.4944\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -3.4424  (n=200 points)\n","  λ(3v8) = -3.4983  (n=200 points)\n","  λ(5v6) = -3.4944  (n=200 points)\n","\n","✓ Model 1/8 complete!\n","  Test accuracy: 0.9875\n","  λ(4v9): -3.4424\n","  λ(3v8): -3.4983\n","  λ(5v6): -3.4944\n","  Training time: 116.8s\n","\n","======================================================================\n","MODEL 2/8: BASELINE (seed=1)\n","======================================================================\n","\n","======================================================================\n","Training: Baseline (no reg) (seed=1)\n","======================================================================\n","  Epoch  3/15: train_loss=1.0807, test_acc=0.9842, reg_loss=0.0000\n","  Epoch  6/15: train_loss=1.0401, test_acc=0.9880, reg_loss=0.0000\n","  Epoch  9/15: train_loss=1.0091, test_acc=0.9866, reg_loss=0.0000\n","  Epoch 12/15: train_loss=0.9731, test_acc=0.9868, reg_loss=0.0000\n","  Epoch 15/15: train_loss=0.9387, test_acc=0.9874, reg_loss=0.0000\n","\n","Training complete! Total time: 117.3s\n","Final test accuracy: 0.9874\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1094 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -3.1943\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1039 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -3.1566\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 762 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -3.1396\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -3.1943  (n=200 points)\n","  λ(3v8) = -3.1566  (n=200 points)\n","  λ(5v6) = -3.1396  (n=200 points)\n","\n","✓ Model 2/8 complete!\n","  Test accuracy: 0.9874\n","  λ(4v9): -3.1943\n","  λ(3v8): -3.1566\n","  λ(5v6): -3.1396\n","  Training time: 117.3s\n","\n","======================================================================\n","MODEL 3/8: GLOBAL (seed=0)\n","======================================================================\n","\n","======================================================================\n","Training: Global regularization (seed=0)\n","======================================================================\n","  Epoch  3/15: train_loss=1.3614, test_acc=0.9695, reg_loss=0.1893\n","  Epoch  6/15: train_loss=1.3371, test_acc=0.9701, reg_loss=0.1834\n","  Epoch  9/15: train_loss=1.3294, test_acc=0.9723, reg_loss=0.1804\n","  Epoch 12/15: train_loss=1.3290, test_acc=0.9633, reg_loss=0.1830\n","  Epoch 15/15: train_loss=1.3309, test_acc=0.9714, reg_loss=0.1861\n","\n","Training complete! Total time: 439.2s\n","Final test accuracy: 0.9714\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1261 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.9585\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1414 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0071\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 1224 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0868\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -4.9585  (n=200 points)\n","  λ(3v8) = -5.0071  (n=200 points)\n","  λ(5v6) = -5.0868  (n=200 points)\n","\n","✓ Model 3/8 complete!\n","  Test accuracy: 0.9714\n","  λ(4v9): -4.9585\n","  λ(3v8): -5.0071\n","  λ(5v6): -5.0868\n","  Training time: 439.2s\n","\n","======================================================================\n","MODEL 4/8: GLOBAL (seed=1)\n","======================================================================\n","\n","======================================================================\n","Training: Global regularization (seed=1)\n","======================================================================\n","  Epoch  3/15: train_loss=1.3584, test_acc=0.9667, reg_loss=0.1910\n","  Epoch  6/15: train_loss=1.3410, test_acc=0.9700, reg_loss=0.1849\n","  Epoch  9/15: train_loss=1.3348, test_acc=0.9702, reg_loss=0.1883\n","  Epoch 12/15: train_loss=1.3315, test_acc=0.9728, reg_loss=0.1881\n","  Epoch 15/15: train_loss=1.3260, test_acc=0.9731, reg_loss=0.1868\n","\n","Training complete! Total time: 438.9s\n","Final test accuracy: 0.9731\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1752 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0744\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1627 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0792\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 1045 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0807\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -5.0744  (n=200 points)\n","  λ(3v8) = -5.0792  (n=200 points)\n","  λ(5v6) = -5.0807  (n=200 points)\n","\n","✓ Model 4/8 complete!\n","  Test accuracy: 0.9731\n","  λ(4v9): -5.0744\n","  λ(3v8): -5.0792\n","  λ(5v6): -5.0807\n","  Training time: 438.9s\n","\n","======================================================================\n","MODEL 5/8: SELECTIVE_49 (seed=0)\n","======================================================================\n","\n","======================================================================\n","Training: Selective 4/9 regularization (seed=0)\n","======================================================================\n","  Epoch  3/15: train_loss=1.3515, test_acc=0.9688, reg_loss=0.1479\n","  Epoch  6/15: train_loss=1.3284, test_acc=0.9681, reg_loss=0.1442\n","  Epoch  9/15: train_loss=1.3175, test_acc=0.9748, reg_loss=0.1426\n","  Epoch 12/15: train_loss=1.3106, test_acc=0.9730, reg_loss=0.1426\n","  Epoch 15/15: train_loss=1.3062, test_acc=0.9733, reg_loss=0.1433\n","\n","Training complete! Total time: 202.3s\n","Final test accuracy: 0.9733\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1638 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.8541\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1336 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.8914\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 1240 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.9032\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -4.8541  (n=200 points)\n","  λ(3v8) = -4.8914  (n=200 points)\n","  λ(5v6) = -4.9032  (n=200 points)\n","\n","✓ Model 5/8 complete!\n","  Test accuracy: 0.9733\n","  λ(4v9): -4.8541\n","  λ(3v8): -4.8914\n","  λ(5v6): -4.9032\n","  Training time: 202.3s\n","\n","======================================================================\n","MODEL 6/8: SELECTIVE_49 (seed=1)\n","======================================================================\n","\n","======================================================================\n","Training: Selective 4/9 regularization (seed=1)\n","======================================================================\n","  Epoch  3/15: train_loss=1.3527, test_acc=0.9667, reg_loss=0.1528\n","  Epoch  6/15: train_loss=1.3218, test_acc=0.9659, reg_loss=0.1453\n","  Epoch  9/15: train_loss=1.3116, test_acc=0.9656, reg_loss=0.1456\n","  Epoch 12/15: train_loss=1.3084, test_acc=0.9731, reg_loss=0.1463\n","  Epoch 15/15: train_loss=1.3004, test_acc=0.9761, reg_loss=0.1436\n","\n","Training complete! Total time: 202.6s\n","Final test accuracy: 0.9761\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1988 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.8653\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1692 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.9997\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 1225 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.9435\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -4.8653  (n=200 points)\n","  λ(3v8) = -4.9997  (n=200 points)\n","  λ(5v6) = -4.9435  (n=200 points)\n","\n","✓ Model 6/8 complete!\n","  Test accuracy: 0.9761\n","  λ(4v9): -4.8653\n","  λ(3v8): -4.9997\n","  λ(5v6): -4.9435\n","  Training time: 202.6s\n","\n","======================================================================\n","MODEL 7/8: SELECTIVE_38 (seed=0)\n","======================================================================\n","\n","======================================================================\n","Training: Selective 3/8 regularization (seed=0)\n","======================================================================\n","  Epoch  3/15: train_loss=1.3618, test_acc=0.9648, reg_loss=0.1809\n","  Epoch  6/15: train_loss=1.3319, test_acc=0.9695, reg_loss=0.1677\n","  Epoch  9/15: train_loss=1.3239, test_acc=0.9747, reg_loss=0.1656\n","  Epoch 12/15: train_loss=1.3192, test_acc=0.9707, reg_loss=0.1693\n","  Epoch 15/15: train_loss=1.3171, test_acc=0.9730, reg_loss=0.1695\n","\n","Training complete! Total time: 202.7s\n","Final test accuracy: 0.9730\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1278 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.8733\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1254 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.8595\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 1567 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0581\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -4.8733  (n=200 points)\n","  λ(3v8) = -4.8595  (n=200 points)\n","  λ(5v6) = -5.0581  (n=200 points)\n","\n","✓ Model 7/8 complete!\n","  Test accuracy: 0.9730\n","  λ(4v9): -4.8733\n","  λ(3v8): -4.8595\n","  λ(5v6): -5.0581\n","  Training time: 202.8s\n","\n","======================================================================\n","MODEL 8/8: SELECTIVE_38 (seed=1)\n","======================================================================\n","\n","======================================================================\n","Training: Selective 3/8 regularization (seed=1)\n","======================================================================\n","  Epoch  3/15: train_loss=1.3674, test_acc=0.9673, reg_loss=0.1818\n","  Epoch  6/15: train_loss=1.3301, test_acc=0.9707, reg_loss=0.1680\n","  Epoch  9/15: train_loss=1.3275, test_acc=0.9679, reg_loss=0.1680\n","  Epoch 12/15: train_loss=1.3225, test_acc=0.9664, reg_loss=0.1696\n","  Epoch 15/15: train_loss=1.3159, test_acc=0.9734, reg_loss=0.1668\n","\n","Training complete! Total time: 204.5s\n","Final test accuracy: 0.9734\n","\n","Measuring λ for all class pairs (wider boundary range)...\n","\n","======================================================================\n","Measuring λ for 3 class pairs (prob range: 0.2-0.8)...\n","======================================================================\n","\n","  Measuring λ for class pair 4 vs 9...\n","  Found 1991 total samples for classes 4 vs 9\n","  Found 1949 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.9936\n","\n","  Measuring λ for class pair 3 vs 8...\n","  Found 1984 total samples for classes 3 vs 8\n","  Found 1572 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -4.9509\n","\n","  Measuring λ for class pair 5 vs 6...\n","  Found 1850 total samples for classes 5 vs 6\n","  Found 1347 boundary points (prob in [0.2, 0.8])\n","  Sampled down to 200 points\n","    Using 200 boundary points\n","    λ = -5.0935\n","\n","======================================================================\n","Lambda Measurement Summary:\n","======================================================================\n","  λ(4v9) = -4.9936  (n=200 points)\n","  λ(3v8) = -4.9509  (n=200 points)\n","  λ(5v6) = -5.0935  (n=200 points)\n","\n","✓ Model 8/8 complete!\n","  Test accuracy: 0.9734\n","  λ(4v9): -4.9936\n","  λ(3v8): -4.9509\n","  λ(5v6): -5.0935\n","  Training time: 204.6s\n","\n","======================================================================\n","EXPERIMENT COMPLETE!\n","======================================================================\n","Total time: 33.1 minutes\n","Average time per model: 248.0s\n","Trained 8 models successfully\n","\n","======================================================================\n","EXPERIMENTAL RESULTS SUMMARY\n","======================================================================\n","\n","Condition            Test Acc     λ(4v9)       λ(3v8)       λ(5v6)      \n","----------------------------------------------------------------------\n","baseline             0.9875±0.0000  -3.3183±0.1241  -3.3274±0.1708  -3.3170±0.1774\n","global               0.9723±0.0008  -5.0164±0.0580  -5.0432±0.0361  -5.0837±0.0031\n","selective_49         0.9747±0.0014  -4.8597±0.0056  -4.9456±0.0542  -4.9233±0.0202\n","selective_38         0.9732±0.0002  -4.9335±0.0602  -4.9052±0.0457  -5.0758±0.0177\n","======================================================================\n"]}]}]}